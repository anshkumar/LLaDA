# Dataset configuration
TTS_dataset: "/workspace/combined_tts_dataset_pretrain"
# text_QA_dataset: "/path/to/your/text/dataset"  # Comment out for TTS-only training

voice_type: "all"

# Model configuration
model_name: "meta-llama/Llama-3.2-3B-Instruct"
tokenizer_name: "meta-llama/Llama-3.2-3B-Instruct"

# Training arguments - Memory optimized for H100 80GB
epochs: 10
batch_size: 8
gradient_accumulation_steps: 1
number_processes: 4
pad_token: 128263
save_epochs: 1  # Save checkpoint every N epochs
learning_rate: 5.0e-5
lr_scheduler_type: "linear"
max_grad_norm: 1.0
max_length: 2048

# Naming and paths
save_folder: "checkpoints_llada_tts"
project_name: "llada-tts-experiment"
run_name: "llada-tts-run-1"

# TTS-specific settings (optional, will use defaults)
# num_audio_tokens: 28672  # 7 * 4096
# num_special_tokens: 10
# max_length: 4096
# mask_token_id: 126336
# eps: 1e-3 